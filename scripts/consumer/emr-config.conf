# EMR Spark Jobs Configuration File
# This file contains all the configuration parameters for EMR Spark jobs

# Default AWS Configuration
DEFAULT_REGION="us-east-1"

# Spark Configuration
SPARK_PACKAGES="org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,software.amazon.s3tables:s3-tables-catalog-for-iceberg-runtime:0.1.4"
SPARK_JARS="/home/hadoop/aws-msk-iam-auth-2.3.2-all.jar"
SPARK_DEPLOY_MODE="client"

# Job Configurations
# Format: JOB_NAME_SCRIPT:JOB_NAME_TABLE:JOB_NAME_TOPIC:JOB_NAME_DATABASE:JOB_NAME_CHECKPOINT:JOB_NAME_DISPLAY_NAME:JOB_NAME_TENANT_REQUIRED
S_DATABASE="$STANDARD_DATABASE"
P_DATABASE="$PREMIUM_DATABASE"

# Network Events Job
NETWORK_SCRIPT="spark-network.py"
NETWORK_TABLE="network_events"
NETWORK_TENANT_REQUIRED="true"
# Standard
NETWORK_TOPIC="networkevents"
NETWORK_CHECKPOINT="$CHECKPOINT_BASE/standard/networkEvents/"
NETWORK_DISPLAY_NAME="Spark Standard Network Events Processing"
# Premium
P_NETWORK_TOPIC="networkevents-p"
P_NETWORK_CHECKPOINT="$CHECKPOINT_BASE/premium/networkEvents/"
P_NETWORK_DISPLAY_NAME="Spark Premium Network Events Processing"

# Standard Authentication Logs Job
AUTH_SCRIPT="spark-auth.py"
AUTH_TABLE="authentication_logs"
AUTH_TENANT_REQUIRED="true"
# Standard
AUTH_TOPIC="authenticationlogs"
AUTH_CHECKPOINT="$CHECKPOINT_BASE/standard/auth/"
AUTH_DISPLAY_NAME="Spark Standard Authentication Logs Processing"
# Premium
P_AUTH_TOPIC="authenticationlogs-p"
P_AUTH_CHECKPOINT="$CHECKPOINT_BASE/premium/auth/"
P_AUTH_DISPLAY_NAME="Spark Premium Authentication Logs Processing"

# Standard Endpoint Job
ENDPOINT_SCRIPT="spark-endpoint.py"
ENDPOINT_TABLE="endpoints"
ENDPOINT_TENANT_REQUIRED="true"
# Standard
ENDPOINT_TOPIC="endpoints"
ENDPOINT_CHECKPOINT="$CHECKPOINT_BASE/standard/endpoint/"
ENDPOINT_DISPLAY_NAME="Spark Standard Endpoint Processing"
# Premium
P_ENDPOINT_TOPIC="endpoints-p"
P_ENDPOINT_CHECKPOINT="$CHECKPOINT_BASE/premium/endpoint/"
P_ENDPOINT_DISPLAY_NAME="Spark Premium Endpoint Processing"


# Standard Software Job
SOFTWARE_SCRIPT="spark-software.py"
SOFTWARE_TABLE="installed_software"
SOFTWARE_TENANT_REQUIRED="true"
# Standard
SOFTWARE_TOPIC="installedsoftware"
SOFTWARE_CHECKPOINT="$CHECKPOINT_BASE/standard/software/"
SOFTWARE_DISPLAY_NAME="Spark Standard Installed Software Processing"
# Premium
P_SOFTWARE_TOPIC="installedsoftware-p"
P_SOFTWARE_CHECKPOINT="$CHECKPOINT_BASE/premium/software/"
P_SOFTWARE_DISPLAY_NAME="Spark Premium Installed Software Processing"

# CVE Job
CVE_SCRIPT="spark-cve.py"
CVE_TABLE="cve_info"
CVE_TOPIC="cveinfo"
CVE_DATABASE="$SHARED_DATABASE"
CVE_CHECKPOINT="$CHECKPOINT_BASE/cveinfo/"
CVE_DISPLAY_NAME="Spark CVE Information Processing"
CVE_TENANT_REQUIRED="false"

# Threat Intelligence Job
THREAT_SCRIPT="spark-threat-intelligence.py"
THREAT_TABLE="threat_intelligence"
THREAT_TOPIC="threatintelligence"
THREAT_DATABASE="$SHARED_DATABASE"
THREAT_CHECKPOINT="$CHECKPOINT_BASE/threat_intelligence/"
THREAT_DISPLAY_NAME="Spark Threat Intelligence Processing"
THREAT_TENANT_REQUIRED="false"

# EMR Step Configuration
ACTION_ON_FAILURE="CONTINUE"
HADOOP_JAR="command-runner.jar"
STEP_CONCURRENCY_LEVEL="14"